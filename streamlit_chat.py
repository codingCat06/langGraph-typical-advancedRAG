"""
Streamlit RAG Chat - ì›¹ UI
LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
"""

import streamlit as st
from rag import stream_response, get_response_with_metadata

st.set_page_config(page_title="RAG Chat", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š RAG Chat (LangGraph)")

# Session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "show_debug" not in st.session_state:
    st.session_state.show_debug = False

# íˆìŠ¤í† ë¦¬ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg.get("debug") and st.session_state.show_debug:
            with st.expander("Debug Info"):
                st.json(msg["debug"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        try:
            if st.session_state.show_debug:
                # Debug ëª¨ë“œ: ì „ì²´ ë©”íƒ€ë°ì´í„°
                result = get_response_with_metadata(prompt)
                full_response = result.get("answer", "")
                response_placeholder.markdown(full_response)
                
                with st.expander("ğŸ” Flow Log"):
                    for entry in result.get("flow_log", []):
                        st.text(entry)
                
                with st.expander("ğŸ“„ Evidence"):
                    for ev in result.get("evidence", []):
                        st.text(f"â€¢ {ev.get('filename')} p.{ev.get('page')}")
                
                debug_info = {
                    "iterations": result.get("iterations"),
                    "sufficiency": str(result.get("sufficiency")),
                    "hallucination": str(result.get("hallucination")),
                }
                
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "debug": debug_info
                })
            else:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                for chunk in stream_response(prompt):
                    full_response += chunk
                    response_placeholder.markdown(full_response + "â–Œ")
                
                response_placeholder.markdown(full_response)
                
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })
                
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ Settings")
    
    st.session_state.show_debug = st.toggle("Debug Mode", st.session_state.show_debug)
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    
    st.subheader("ğŸ“Š Pipeline Info")
    st.caption("""
    - Planner: Self-Query
    - Retrieval: ANN (Chroma)
    - Rerank: LLM-based
    - Sufficiency: Loop if needed
    - Hallucination: Grounding check
    """)
    
    st.divider()
    st.caption("CLI: `python main.py chat`")
