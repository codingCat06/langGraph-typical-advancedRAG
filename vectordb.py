"""
VectorDB - ChromaDB ì €ì¥ ë° ê´€ë¦¬
í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ í¬í•¨
ë ˆê±°ì‹œ/ì‹ ê·œ store.json í¬ë§· ëª¨ë‘ ì§€ì›
"""

import json
import time
import re
from typing import Dict, Any, List

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from settings import STORE_FILE, CHROMA_DIR


# =============================================================================
# Store Management (í˜¸í™˜ì„± ë ˆì´ì–´)
# =============================================================================

def load_store() -> Dict[str, Any]:
    if STORE_FILE.exists():
        with open(STORE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_store(store: Dict[str, Any]) -> None:
    with open(STORE_FILE, "w", encoding="utf-8") as f:
        json.dump(store, f, ensure_ascii=False, indent=2)


def get_doc_entries(store: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    ë¬¸ì„œ í•­ëª©ë§Œ ì¶”ì¶œ (ë ˆê±°ì‹œ/ì‹ ê·œ í¬ë§· í˜¸í™˜)
    Returns: {url: data} ë”•ì…”ë„ˆë¦¬
    """
    entries = {}
    
    # ì‹ ê·œ í¬ë§·: documents í‚¤ ì•„ë˜ì— ì €ì¥
    if "documents" in store:
        for doc_id, data in store.get("documents", {}).items():
            if isinstance(data, dict):
                url = data.get("source_url", doc_id)
                entries[url] = data
    
    # ë ˆê±°ì‹œ í¬ë§·: URLì´ ì§ì ‘ í‚¤ë¡œ ì €ì¥
    for key, data in store.items():
        if key in ("documents", "cache", "events", "_legacy_migrated"):
            continue
        if isinstance(data, dict) and key.startswith("http"):
            entries[key] = data
    
    return entries


def update_doc_entry(store: Dict[str, Any], url: str, updates: Dict[str, Any]) -> None:
    """
    ë¬¸ì„œ í•­ëª© ì—…ë°ì´íŠ¸ (ë ˆê±°ì‹œ/ì‹ ê·œ í¬ë§· í˜¸í™˜)
    """
    # ë ˆê±°ì‹œ: ë£¨íŠ¸ ë ˆë²¨ì— ìˆìœ¼ë©´ ê·¸ê³³ ì—…ë°ì´íŠ¸
    if url in store:
        store[url].update(updates)
        return
    
    # ì‹ ê·œ: documents ì•„ë˜ì—ì„œ ì°¾ê¸°
    if "documents" in store:
        for doc_id, data in store["documents"].items():
            if data.get("source_url") == url:
                store["documents"][doc_id].update(updates)
                return
    
    # ëª» ì°¾ìœ¼ë©´ ë ˆê±°ì‹œ ë°©ì‹ìœ¼ë¡œ ì¶”ê°€
    if url not in store:
        store[url] = {}
    store[url].update(updates)


# =============================================================================
# Page Extraction
# =============================================================================

def extract_page_from_content(content: str, position: int) -> int:
    """
    ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì—ì„œ position ì´ì „ê¹Œì§€ì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    <span id="page-X-Y"> í˜•ì‹ì—ì„œ Xê°€ í˜ì´ì§€ ë²ˆí˜¸ (0-indexed)
    """
    text_before = content[:position]
    page_matches = re.findall(r'<span id="page-(\d+)-\d+">', text_before)
    
    if page_matches:
        return int(page_matches[-1]) + 1  # 0-indexed -> 1-indexed
    return 1  # ê¸°ë³¸ê°’


def split_with_page_info(content: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ë©´ì„œ ê° ì²­í¬ì˜ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
    """
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    chunks_with_info = []
    current_pos = 0
    
    chunks = splitter.split_text(content)
    
    for chunk in chunks:
        chunk_start = content.find(chunk[:50], current_pos)
        if chunk_start == -1:
            chunk_start = current_pos
        
        page_num = extract_page_from_content(content, chunk_start)
        
        chunks_with_info.append({
            "text": chunk,
            "page": page_num
        })
        
        current_pos = chunk_start + len(chunk) - chunk_overlap
    
    return chunks_with_info


# =============================================================================
# Functions
# =============================================================================

def get_pending() -> List[Dict[str, Any]]:
    """vectorized=False, status=successì¸ í•­ëª©"""
    store = load_store()
    entries = get_doc_entries(store)
    
    return [
        {"url": url, **data}
        for url, data in entries.items()
        if data.get("status") == "success" 
        and data.get("parsed_file")
        and not data.get("vectorized", False)
    ]


def update() -> int:
    """pending ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥ (í˜ì´ì§€ ì •ë³´ í¬í•¨)"""
    pending = get_pending()
    
    if not pending:
        print("[INFO] ë²¡í„°í™”í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    print(f"[UPDATE] {len(pending)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    
    all_chunks = []
    all_metadatas = []
    processed_urls = []
    
    for doc in pending:
        parsed_file = doc.get("parsed_file")
        url = doc.get("url")
        
        filename = url.split("/")[-1] if "/" in url else url
        
        try:
            with open(parsed_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            chunks_info = split_with_page_info(content)
            
            for i, chunk_info in enumerate(chunks_info):
                all_chunks.append(chunk_info["text"])
                all_metadatas.append({
                    "source": url,
                    "filename": filename,
                    "page": chunk_info["page"],
                    "chunk_index": i,
                    "total_chunks": len(chunks_info),
                    "page_count": doc.get("page_count"),
                })
            
            processed_urls.append(url)
            print(f"  âœ… {filename}: {len(chunks_info)} chunks")
            
        except Exception as e:
            print(f"  âŒ {e}")
    
    if not all_chunks:
        return 0
    
    print(f"[CHROMA] {len(all_chunks)}ê°œ ì²­í¬ ì €ì¥ ì¤‘...")
    
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    if CHROMA_DIR.exists():
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DIR),
            embedding_function=embeddings
        )
        vectorstore.add_texts(texts=all_chunks, metadatas=all_metadatas)
    else:
        Chroma.from_texts(
            texts=all_chunks,
            embedding=embeddings,
            metadatas=all_metadatas,
            persist_directory=str(CHROMA_DIR)
        )
    
    # Mark as vectorized
    store = load_store()
    for url in processed_urls:
        update_doc_entry(store, url, {
            "vectorized": True,
            "vectorized_at": time.strftime("%Y-%m-%d %H:%M:%S")
        })
    save_store(store)
    
    print(f"[SUCCESS] {len(all_chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
    return len(all_chunks)


def reset(urls: List[str] = None) -> None:
    """vectorized ìƒíƒœ ë¦¬ì…‹ + ChromaDBì—ì„œ ì‚­ì œ"""
    store = load_store()
    entries = get_doc_entries(store)
    
    # ChromaDBì—ì„œ ì‚­ì œ
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DIR),
            embedding_function=embeddings
        )
        
        if urls:
            for url in urls:
                vectorstore._collection.delete(where={"source": url})
        else:
            vectorstore.delete_collection()
        
        print("[CHROMA] VectorDBì—ì„œ ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"[WARN] VectorDB ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # Store ì—…ë°ì´íŠ¸
    target_urls = urls if urls else list(entries.keys())
    for url in target_urls:
        update_doc_entry(store, url, {
            "vectorized": False,
            "vectorized_at": None
        })
    
    save_store(store)
    print("[RESET] ì™„ë£Œ")


def list_vectorized() -> None:
    """ë²¡í„°í™”ëœ ë¬¸ì„œ ëª©ë¡ ì¶œë ¥"""
    store = load_store()
    entries = get_doc_entries(store)
    
    vectorized = [
        (url, data) for url, data in entries.items()
        if data.get("vectorized", False)
    ]
    
    if not vectorized:
        print("[INFO] ë²¡í„°í™”ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n{'='*50}")
    print(f"VectorDB ë¬¸ì„œ ëª©ë¡ ({len(vectorized)}ê°œ)")
    print(f"{'='*50}\n")
    
    for url, data in vectorized:
        filename = url.split("/")[-1] if "/" in url else url
        print(f"ğŸ“„ {filename}")
        print(f"   URL: {url[:50]}...")
        print(f"   ì €ì¥ì¼: {data.get('vectorized_at', 'N/A')}")
        print(f"   í˜ì´ì§€: {data.get('page_count', 'N/A')}p")
        print()


def get_retriever(k: int = 3):
    """ChromaDB retriever ë°˜í™˜"""
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma(
        persist_directory=str(CHROMA_DIR),
        embedding_function=embeddings
    )
    return vectorstore.as_retriever(search_kwargs={"k": k})


def get_vectorstore():
    """ChromaDB vectorstore ë°˜í™˜"""
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    return Chroma(
        persist_directory=str(CHROMA_DIR),
        embedding_function=embeddings
    )
